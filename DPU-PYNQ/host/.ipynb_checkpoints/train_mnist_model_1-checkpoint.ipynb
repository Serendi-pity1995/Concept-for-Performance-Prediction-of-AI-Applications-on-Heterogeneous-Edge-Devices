{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN for DPU compilation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim/s\n",
    "\n",
    "In this notebook we show how to train a simple Convolutional Neural Network (CNN)\n",
    "on the MNIST handwritten digit dataset for deployment on the DPU. We will cover:\n",
    "\n",
    "* Loading and pre-processing the imagenet dataset\n",
    "* Training a CNN with Tensorflow2\n",
    "* Quantizing and evaluating the quantized model\n",
    "* Compiling for DPU using the Vitis AI compiler\n",
    "\n",
    "## References\n",
    "\n",
    "* [Vitis AI model zoo](https://github.com/Xilinx/Vitis-AI/tree/master/models/AI-Model-Zoo)\n",
    "\n",
    "## Last revised\n",
    "* Dec 13, 2021\n",
    "    * Initial revision\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 13:44:08.794538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-03-03 13:44:08.794578: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset\n",
    "\n",
    "The MNIST dataset comes with 60k training and 10k test examples that are\n",
    "28x28 grayscale images, along with their labels which are just the \n",
    "corresponding digits saved as integers. We can use the `keras.datasets`\n",
    "utility to load the MNIST dataset straight into our Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 28, 28), (60000,)\n",
      "Test data: (10000, 28, 28), (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Training data: {}, {}'.format(x_train.shape, y_train.shape))\n",
    "print('Test data: {}, {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr0AAACTCAYAAACDHaG3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbJUlEQVR4nO3de7SVVb3G8ecnimQcRBSJIEANKXN4SVAij1KAmmmm5oWRXMwTjrxEDeWghkanMPJ2Dl7TCFBkiI7AUMvQoaCZykENCxTE7EBc5Cp3itB5/liLtzlf9tp77Xevy7ve9f2MsQe/ueda651r72ds5l5r7jnNOScAAAAgy/ap9gAAAACAcmPSCwAAgMxj0gsAAIDMY9ILAACAzGPSCwAAgMxj0gsAAIDMY9IbY2Zzzew/Kn1f1DZygyTIDZqLzCAJcpOT2Umvmf2fmQ2s9jgKMbPhZvahmW3zPvpXe1z1Lu25kSQz+76ZvW9mm81skpntX+0x1btayM0eZva8mTkz27faY6lnac+MmR1tZrPNbL2ZsaF/StRAbvY3s/82s1Vm9oGZ3Wtm+1V7XHtkdtJbI15xzrX1PuZWe0BINzM7XdJ1kgZI6iHpcEk/quaYUDvM7JuSmOyiGP+U9Jiky6o9ENSU6yT1lnS0pCMlfV7SmKqOyFN3k14zO8jMnjKzdfnfQp4ys66xmx1hZv+bfyVtlpl18O7f18xeNrNNZvYmr87WhxTlZpikXzrnFjnnPpD0Y0nDEz4WyixFuZGZHSjph5L+M+ljoPzSkhnn3BLn3C8lLUr+bFApacmNpLMl3emc2+icWyfpTknfSvhYJVd3k17lnvNkSd0ldZO0U9LdsdsMVe6b9ElJu5X7psnMukj6jaSfSOog6VpJM8ysY/wiZtYtH55ujYzl+PxbR++Y2Y283ZhqacnN5yS96bXflNTJzA5O+LxQXmnJjSTdLOk+Se+35Amh7NKUGdSOtOTG8h9+u2v+l+6qq7tJr3Nug3NuhnNuh3Nuq6Rxkk6N3Wyqc26hc267pBslXWhmrSRdIum3zrnfOuc+cs49K+k1SWc2cJ3lzrn2zrnlBYbyonIv/x8q6XxJgyWNKsmTRMmlKDdtJW322nvqf2vB00OZpCU3ZtZb0hcl3VXCp4cySEtmUFtSlJunJY00s45m9glJ381//oASPM0Wq7tJr5kdYGb3m9kyM9ui3OSzff4bv8ffvHqZpP0kHaLcb1AX5H/L2WRmmySdLKlzc8fhnHvPOffXfMD+LOm/JH0j4dNCmaUlN5K2SWrntffUWxM8FsosDbkxs30k3StppHNudwueDiogDZlB7UlRbsZJ+qOkBZJelvRr5daHr03wWCVXd5NeSddI6iXpJOdcO0mn5D/vvxz/Ka/uptw3bL1ygZma/y1nz8fHnXPjSzAuFxsD0iUtuVkk6VivfaykNc65DQkeC+WXhty0U+4PSx41s/clzc9/foWZ/XszHwvll4bMoPakIjfOuZ3Ouaucc12cc4dL2iDpdefch0meVKllfdK7n5m18T72Ve5t4J2SNuUXcf+wgftdYmZHmdkByr0C+6v8N+xhSWeb2elm1ir/mP0bWCzeJDP7ipl1ytefUe6thlkJnydKK7W5kfSQpMvy1zlIub+KnZLkSaLk0pqbzcqt4Tsu/7HnLcsTJM1r7pNESaU1M7KcNpJa59ttjO0R0yLNueliZp/M56evcnObhsZSFVmf9P5WuRDs+Rgr6X8kfUy5325elfS7Bu43VbmJxPuS2ii/JsU59zdJ50i6QdI65X47GqUGvo6WW+y9zQov9h4g6U9mtj0/zpnK/aEJqi+1uXHO/U7SLZLmKPf21DKl6AdKnUtlblzO+3s+8o8l5d4h2JXwuaI0UpmZvO75Me3ZvWGnpCXNe3ookzTn5gjlljVsl/SgpOucc880/ymWhznHntMAAADItqy/0gsAAAAw6QUAAED2MekFAABA5rVo0mtmZ5jZEjN718yuK9WgkG3kBkmQGyRBbpAEucmmxH/Ilt/w+B1JgyStUG7vx8HOubdKNzxkDblBEuQGSZAbJEFusmvfFtz3REnvOufekyQzm67clhcFQ2FmbBWRMs65Sh+IQW4ygNwgibTnhsyk0nrnXMcKX5Pc1LhCP2tasryhi8Ij7VbkPwc0htwgCXKDJMhN7VtWhWuSm4xqySu9Dc2i9/ptx8xGSBrRgusgW8gNkiA3SKLJ3JAZNIDcZFRLJr0rFJ7j3FXSqviNnHMPSHpA4i0ASCI3SIbcIIkmc0Nm0AByk1EtWd4wX1JPMzvMzFpLuljSE6UZFjKM3CAJcoMkyA2SIDcZlfiVXufcbjO7StJsSa0kTXLOLWribqhz5AZJkBskQW6QBLnJrsRbliW6GG8BpE4V/pq62chN+pAbJJH23JCZVHrdOde72oNoDLlJn3Ls3gAAAADUBCa9AAAAyDwmvQAAAMg8Jr0AAADIPCa9AAAAyDwmvQAAAMi8lpzIBqAFTjjhhKB91VVXRfXQoUODvoceeiho33XXXVH9xhtvlGF0AABkC6/0AgAAIPOY9AIAACDzmPQCAAAg8ziGuIBWrVoF7QMPPLDo+/prMw844ICgr1evXkH7yiuvjOrbbrst6Bs8eHBU//3vfw/6xo8fH7R/9KMfFT0+X9qPBZVqKzeNOe6444L2888/H7TbtWtX9GNt3rw5qg8++OAWjSsJclP7BgwYELSnTZsW1aeeemrQt2TJkpJcM+25ITPSmDFjgrb/f8s++4Svk/Xv3z+qX3jhhXINiWOI0WwcQwwAAIC6xaQXAAAAmcekFwAAAJmX+X16u3XrFtWtW7cO+vr16xe0Tz755Khu37590Hf++eeXZDwrVqwI2nfeeWdUn3vuuUHf1q1bo/rNN98M+sq4fgoldOKJJ0b1jBkzgr74OnF/fb3/vZekXbt2BW1/HW/fvn2DPn/f3vj90LRTTjklaMfXTD/++OOVHE7Z9OnTJ2jPnz+/SiNBNQ0fPjxojx49Omh/9NFHBe9byb8JAkqBV3oBAACQeUx6AQAAkHmZW97Q2LZQzdl2rFTibw3Ft4PZtm1bVPtbBknS6tWro/qDDz4I+kq1hRBazt+W7vOf/3zQ9/DDD0d1586di37MpUuXBu1bbrklaE+fPj2q//CHPwR9fsZ++tOfFn1N5PjbMElSz549g3YtL2/wt5w67LDDgr7u3btHtVmqdxZDCfnfd0lq06ZNlUaCSjjppJOC9iWXXBLV8a0KP/e5zxV8nGuvvTZor1q1Kmj7y0X9/wclad68ecUNtgx4pRcAAACZx6QXAAAAmcekFwAAAJmXuTW9y5cvD9obNmyI6lKt6Y2vR9m0aVPQ/tKXvhTV8S2jpk6dWpIxID3uv//+qPaPjm6J+Nrgtm3bBm1/y7r4GtRjjjmmJGOoV0OHDg3ar7zySpVGUnr+uvJvf/vbQZ+/7m7x4sUVGxMqb+DAgVF99dVXN3pbPwtnnXVW0LdmzZrSDgxlcdFFF0X1hAkTgr5DDjkkquNr+efOnRu0O3bsGNW33npro9f0H8u/nyRdfPHFjQ+4jHilFwAAAJnHpBcAAACZl7nlDRs3bgzao0aNiur4WzN//OMfg7Z/OlrcggULonrQoEFB3/bt24O2v83HyJEjGx8was4JJ5wQtL/61a9GdWNbPcVP0XvyySeD9m233RbV8e1f4ln1t7D78pe/HPSx3VTL+Nt6Zc3EiRML9sW3yUN2+NtHSdLkyZOjuqllf/7b2MuWLSvtwFAS++4bTuV69+4dtH/xi19Etb/FpiS9+OKLUf3jH/846HvppZeC9v777x/Vjz32WNB32mmnFRzfa6+9VrCv0rL70x0AAADIY9ILAACAzGty0mtmk8xsrZkt9D7XwcyeNbOl+X8PKu8wUWvIDZIgN0iC3CAJclN/zDnX+A3MTpG0TdJDzrmj85+7RdJG59x4M7tO0kHOudFNXsys8YuVWbt27YL21q1bg7a/9dRll10W9PlH9T3yyCNlGF11OOfKsgA0S7lp7Ghrae9c+Z5++umojm9nFj/y0d9qLL72ct26dQWv8eGHHwbtHTt2FLzGG2+8UfBxmiNrufG/9vEtymbOnBm0hwwZUuzDps7LL78c1X379g36+vXrF9WvvvpqWa6f9txU+2dNufhrOiXpW9/6VsHbxrepGjBgQDmG1ByvO+d6N32z5stKboYPHx60G1u7/+yzzwZtfzuzLVu2NHodfx40ZcqURm+7cuXKqI6vMW7s/7NSKfSzpslXep1zL0raGPv0OZIezNcPSvp6SwaH7CE3SILcIAlygyTITf1JuntDJ+fcaklyzq02s0ML3dDMRkgakfA6yBZygyTIDZIoKjdkBjHkJsPKvmWZc+4BSQ9I1X8LALWD3CAJcoPmIjNIgtzUpqST3jVm1jn/W1BnSWtLOahyaWq9yubNmwv2+Ud2Pvroo0HfRx991LKB1Y+ayc2RRx4Z1f5ez9Le+1quX78+qlevXh30Pfjgg1G9bdu2oO83v/lNo+2kPvaxj0X1NddcE/R985vfLMk1KqzsuTnzzDOj2v/61bpOnToF7cMOO6zgbf01eBlRMz9vSs0/Wlbaew2v/3/Wpk2bgr6f/OQnZRtXjaiJ3Ph76t5www1BX/xvte69996oHjNmTNDX1LzI94Mf/KDo2373u9+N6kqs4S1W0i3LnpA0LF8PkzSrNMNBxpEbJEFukAS5QRLkJsOK2bLsEUmvSOplZivM7DJJ4yUNMrOlkgbl20CE3CAJcoMkyA2SIDf1p8nlDc65wQW6qr6PSamNHTs2quNHzfpbPw0cODDoe+aZZ8o6rlpUa7nxj1eUwiOB/be+pb23uhs6dGhUx49brPZb5d26davq9ZurWrnp1atXwb5FixaV89Jl5edYCpc7vPPOO0FfPNe1pNZ+3pRDjx49onrGjBlF3++uu+4K2nPmzCnVkFKvlnJz0003BW1/ScOuXbuCvtmzZwft0aP/tePazp07C16jTZs2QTt+tLD//0n8uPv4sphZs9L5AjknsgEAACDzmPQCAAAg85j0AgAAIPPKvk9vLdm+fXtU+1uUSeHxrfEjHeNroPx1nffcc0/Q19Sxz6iO448/PmjH1/H6zjnnnKD9wgsvlGVMSIf58+dXewiB+LHXZ5xxRlT7x4RKe6/J8/lbHkl7b12F2uLnwD9WuyHPPfdcVE+YMKFsY0LLtG/fPqqvuOKKoM+fS8TX8H79618v+hqf/vSno3ratGlBX/xvm3y/+tWvgvYtt9xS9DWriVd6AQAAkHlMegEAAJB5LG8o4C9/+UvQHj58eFRPnjw56BsyZEjB9sc//vGg76GHHgra8RO8UB133HFH0Pa3Y4kvX0jbcoZ99gl/d+WEwNLq0KFDovsde+yxQTu+xY+/9WHXrl2DvtatW0d1/BS9+Pfb34Jo3rx5Qd8//vGPoL3vvv/6kf/6668XHDvSL/4W9vjxhbeTfemll4L2sGHDorqxk0hRXf7Pgfgpez7/9DNJOvTQQ4P2pZdeGtVf+9rXgr6jjz46qtu2bRv0xZdj+u2HH3446POXh6YZr/QCAAAg85j0AgAAIPOY9AIAACDzWNNbpMcffzyqly5dGvTF14MOGPCvEwxvvvnmoK979+5Be9y4cVG9cuXKFo8TxTvrrLOi+rjjjgv6/LVLTzzxRKWGlEh8Da8/9gULFlR4NLXJXxcbX8f285//PGj7x382Jr5tVHxN7+7du6N6x44dQd9bb70V1ZMmTQr64kdd+2vM16xZE/StWLEiaPvHYi9evLjg2JFOSY8afu+994J2PCdIJ/944XXr1gV9HTt2jOq//vWvQV9ztkZdtWpVVG/ZsiXo69y5c9Bev359VD/55JNFXyNNeKUXAAAAmcekFwAAAJnHpBcAAACZx5reBBYuXBi0L7zwwqB99tlnR3V8T9/LL788aPfs2TOqBw0aVKohogj++kZ/P0RJWrt2bVQ/+uijFRtTIfvvv3/QHjt2bMHbPv/881F9/fXXl2tImeIf8bls2bKgr1+/fokec/ny5UH717/+ddB+++23o/rVV19NdI24ESNGBG1/3Z+099pO1JbRo0dHdXP2425sD1+kl380eHxf5qeeeiqq43uJx88ZmDVrVlRPmTIl6Nu4cWNUT58+PeiLr+mN99ciXukFAABA5jHpBQAAQOaxvKEE/LcgJGnq1KlRPXHixKDPPwZUkk455ZSo7t+/f9A3d+7ckowPzecf31qNo6LjyxnGjBkTtEeNGhXV8W2pbr/99qjetm1bGUaXbT/72c+qPYTE/O0SG9Kcba5QffGtFE877bSi7ue/nS1JS5YsKdWQUCXxI8bjS5eS8ucgp556atAXX0KTheVRvNILAACAzGPSCwAAgMxj0gsAAIDMY01vAvHjRb/xjW8E7T59+kR1fA1vnH/c6IsvvliC0aEUqnH0sL9+z1+zK0kXXXRR0PbX7J1//vllHReywz9OHen3zDPPBO2DDjqo4G39be+GDx9eriEhY/ytOxs70l5iyzIAAACgJjDpBQAAQOYx6QUAAEDmsaa3gF69egXtq666KqrPO++8oO8Tn/hE0Y/74YcfBm1/D9jmHCuJljOzBmspPPJx5MiRZbn+97///aB94403RvWBBx4Y9E2bNi1oDx06tCxjApAeBx98cNBu7P+Ie++9N6rZnxvFmj17drWHUFFNvtJrZp8yszlm9raZLTKzkfnPdzCzZ81saf7fwivsUXfIDZIgN2guMoMkyE19KmZ5w25J1zjnPiupr6QrzewoSddJes4511PSc/k2sAe5QRLkBs1FZpAEualDTS5vcM6tlrQ6X281s7cldZF0jqT++Zs9KGmupNFlGWWZxJclDB48OKr95QyS1KNHj0TXeO2114L2uHHjgnY1tsaqhFrIjb8dS3xrFj8bd955Z9A3adKkoL1hw4ao7tu3b9A3ZMiQqD722GODvq5duwbt5cuXR3X8LSf/rcssq4Xc1JL4sp0jjzwyqv0trmpZljIzefLkoL3PPsX/2c3LL79c6uFkWpZy0xKnn356tYdQUc36QzYz6yHpeEnzJHXKh2ZPeA4t+eiQCeQGSZAbNBeZQRLkpn4U/YdsZtZW0gxJ33PObYm/gtDI/UZIGpFseKh15AZJkBs0F5lBEuSmvhT1Sq+Z7adcKKY552bmP73GzDrn+ztLWtvQfZ1zDzjnejvnepdiwKgd5AZJkBs0F5lBEuSm/jT5Sq/lfu35paS3nXN3eF1PSBomaXz+31kN3L3qOnXqFNVHHXVU0Hf33XcH7c985jOJrjFv3rygfeutt0a1f1ysVD/bktV6blq1ahXVV1xxRdAXP/Z3y5YtUd2zZ8+irxFfgzdnzpyovummm4p+nCyp9dykTXytenPWiNaKWs+Mf/z4wIEDg774/xe7du2K6nvuuSfoW7NmTekHl2G1nptSOfzww6s9hIoqZnnDFyUNkfRnM1uQ/9wNygXiMTO7TNJySReUZYSoVeQGSZAbNBeZQRLkpg4Vs3vDS5IKLXIZUNrhICvIDZIgN2guMoMkyE19ysSJbB06dIjq+++/P+jz3zpqycv4/lvRt99+e9AX315q586dia+DynnllVeiev78+UFfnz59Ct4vvtWdv4Qmzt/ObPr06UFfuU56Awr5whe+ENVTpkyp3kAQad++fVQ3dbrnypUro/raa68t15BQR37/+99HdXz5UxaXY2ZvgRcAAAAQw6QXAAAAmcekFwAAAJlXE2t6TzrppKA9atSooH3iiSdGdZcuXRJfZ8eOHVEdP3r25ptvjurt27cnvgbSY8WKFVF93nnnBX2XX355VI8ZM6box5wwYULQvu+++6L63Xffbe4QgRYpdqN9APVp4cKFUb106dKgL/53UEcccURUr1u3rrwDKxNe6QUAAEDmMekFAABA5jHpBQAAQObVxJrec889t9F2Y956662ofuqpp4K+3bt3B21//91NmzY1Y4SodatXrw7aY8eObbAG0uzpp58O2hdcwGFSabd48eKojh9NfvLJJ1d6OKhj/t8uSdLEiROD9rhx46L66quvDvr8uVaa8UovAAAAMo9JLwAAADLPnHOVu5hZ5S6GojjnUr+nEblJH3KDJNKeGzKTSq8753pXexCNyUpu2rVrF7Qfe+yxoD1w4MConjlzZtB36aWXBu1qb+1a6GcNr/QCAAAg85j0AgAAIPOY9AIAACDzWNNb59K+xk4iN2lEbpBE2nNDZlKJNb1VEl/j629Z9p3vfCfoO+aYY4J2tbcwY00vAAAA6haTXgAAAGQeyxvqXNrfbpTITRqRGySR9tyQmVRieQOajeUNAAAAqFtMegEAAJB5THoBAACQeftW+HrrJS2TdEi+ToM0jUWq7Hi6V+g6LUVumkZu9rZe0nbV7/epGOQmlMafNVJ9j4fcJFev4ymYmYr+IVt0UbPX0rIwPU1jkdI3njRJ09cmTWOR0jeetEjb14Xx1Ia0fV0YT21I29eF8eyN5Q0AAADIPCa9AAAAyLxqTXofqNJ1G5KmsUjpG0+apOlrk6axSOkbT1qk7evCeGpD2r4ujKc2pO3rwnhiqrKmFwAAAKgkljcAAAAg8yo66TWzM8xsiZm9a2bXVfLa+etPMrO1ZrbQ+1wHM3vWzJbm/z2oguP5lJnNMbO3zWyRmY2s9pjSiNwEYyEzRSI3wVjITZHITTAWclMkchOMJbW5qdik18xaSbpH0lckHSVpsJkdVanr502RdEbsc9dJes4511PSc/l2peyWdI1z7rOS+kq6Mv81qeaYUoXc7IXMFIHc7IXcFIHc7IXcFIHc7CW9uXHOVeRD0hckzfba10u6vlLX967bQ9JCr71EUud83VnSkkqPyRvLLEmD0jSman+QGzJDbsgNuUnH94nckJtaz00llzd0kfQ3r70i/7lq6+ScWy1J+X8PrcYgzKyHpOMlzUvLmFKC3BRAZhpFbgogN40iNwWQm0aRmwLSlptKTnqtgc+xdYQkM2sraYak7znntlR7PClDbhpAZppEbhpAbppEbhpAbppEbhqQxtxUctK7QtKnvHZXSasqeP1C1phZZ0nK/7u2khc3s/2UC8U059zMNIwpZchNDJkpCrmJITdFITcx5KYo5CYmrbmp5KR3vqSeZnaYmbWWdLGkJyp4/UKekDQsXw9Tbu1JRZiZSfqlpLedc3ekYUwpRG48ZKZo5MZDbopGbjzkpmjkxpPq3FR4MfOZkt6R9BdJP6jCYupHJK2W9E/lfjO7TNLByv0V4dL8vx0qOJ6TlXsL5E+SFuQ/zqzmmNL4QW7IDLkhN+SG3KT5g9zURm44kQ0AAACZx4lsAAAAyDwmvQAAAMg8Jr0AAADIPCa9AAAAyDwmvQAAAMg8Jr0AAADIPCa9AAAAyDwmvQAAAMi8/weTn6ERINA94AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].imshow(x_train[i], 'gray')\n",
    "    axs[i].set_title('Label: {}'.format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will normalize the training and test images. We also need to add\n",
    "color channel. All these pre-processing steps are required by Conv2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:09:25.604446: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2400000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "x_train = tf.image.resize(x_train, [100,100])\n",
    "x_test = tf.image.resize(x_test, [100,100])\n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model\n",
    "\n",
    "Create a `tensorflow.keras.model` object by passing a list of layers. Because the MNIST dataset is not difficult, we can use a very simple network with a single convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 76832)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               9834624   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 9,836,234\n",
      "Trainable params: 9,836,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(100,100,1))\n",
    "x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_classifier')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_88 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# 卷积层和最大池化层\n",
    "conv1 = Conv2D(64, (3,3), padding='same', activation='relu')(inputs)\n",
    "conv2 = Conv2D(64, (3,3), padding='same', activation='relu')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=2)(conv2)\n",
    "\n",
    "conv3 = Conv2D(128, (3,3), padding='same', activation='relu')(pool1)\n",
    "conv4 = Conv2D(128, (3,3), padding='same', activation='relu')(conv3)\n",
    "pool2 = MaxPooling2D(pool_size=2)(conv4)\n",
    "\n",
    "conv5 = Conv2D(256, (3,3), padding='same', activation='relu')(pool2)\n",
    "conv6 = Conv2D(256, (3,3), padding='same', activation='relu')(conv5)\n",
    "conv7 = Conv2D(256, (3,3), padding='same', activation='relu')(conv6)\n",
    "pool3 = MaxPooling2D(pool_size=2)(conv7)\n",
    "\n",
    "conv8 = Conv2D(512, (3,3), padding='same', activation='relu')(pool3)\n",
    "conv9 = Conv2D(512, (3,3), padding='same', activation='relu')(conv8)\n",
    "conv10 = Conv2D(512, (3,3), padding='same', activation='relu')(conv9)\n",
    "pool4 = MaxPooling2D(pool_size=2)(conv10)\n",
    "\n",
    "conv11 = Conv2D(512, (3,3), padding='same', activation='relu')(pool4)\n",
    "conv12 = Conv2D(512, (3,3), padding='same', activation='relu')(conv11)\n",
    "conv13 = Conv2D(512, (3,3), padding='same', activation='relu')(conv12)\n",
    "pool5 = MaxPooling2D(pool_size=2)(conv13)\n",
    "\n",
    "# 扁平层\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# 全联接层\n",
    "fc1 = Dense(4096, activation='relu')(flat)\n",
    "fc2 = Dense(4096, activation='relu')(fc1)\n",
    "\n",
    "# 输出层\n",
    "outputs = Dense(1000, activation='softmax')(fc2)\n",
    "\n",
    "\n",
    "my_VGG16_model = Model(inputs=inputs, outputs=outputs)\n",
    "my_VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 40)        3280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 6, 6, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               184448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 189,018\n",
      "Trainable params: 189,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 04:56:44.522857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-02-22 04:56:44.522896: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-22 04:56:44.522927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-02-22 04:56:44.523136: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28,1))\n",
    "x = tf.keras.layers.Conv2D(40, (9,9), activation='relu', input_shape=(28,28,1))(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((3,3))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_classifier')\n",
    "model.summary( print_fn=None)\n",
    "model.count_params()\n",
    "model.get_config()\n",
    "model.output\n",
    "#model.get_layer(Conv2D)\n",
    "#model.compute_output_shape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 13, 13, 32) dtype=float32 (created by layer 'max_pooling2d')>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the model for training: choose desired optimizer, loss function and metrics to observe over the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 09:14:03.159277: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-03-03 09:14:03.159465: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2022-03-03 09:14:03.163550: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.009ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/30.96b flops)\n",
      "  model_6/conv2d_79/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_81/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_87/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_86/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_83/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_84/Conv2D (3.70b/3.70b flops)\n",
      "  model_6/conv2d_80/Conv2D (1.85b/1.85b flops)\n",
      "  model_6/conv2d_82/Conv2D (1.85b/1.85b flops)\n",
      "  model_6/conv2d_85/Conv2D (1.85b/1.85b flops)\n",
      "  model_6/conv2d_90/Conv2D (924.84m/924.84m flops)\n",
      "  model_6/conv2d_89/Conv2D (924.84m/924.84m flops)\n",
      "  model_6/conv2d_88/Conv2D (924.84m/924.84m flops)\n",
      "  model_6/dense_18/MatMul (205.52m/205.52m flops)\n",
      "  model_6/conv2d_78/Conv2D (173.41m/173.41m flops)\n",
      "  model_6/dense_19/MatMul (33.55m/33.55m flops)\n",
      "  model_6/dense_20/MatMul (8.19m/8.19m flops)\n",
      "  model_6/max_pooling2d_30/MaxPool (3.21m/3.21m flops)\n",
      "  model_6/conv2d_78/BiasAdd (3.21m/3.21m flops)\n",
      "  model_6/conv2d_79/BiasAdd (3.21m/3.21m flops)\n",
      "  model_6/max_pooling2d_31/MaxPool (1.61m/1.61m flops)\n",
      "  model_6/conv2d_81/BiasAdd (1.61m/1.61m flops)\n",
      "  model_6/conv2d_80/BiasAdd (1.61m/1.61m flops)\n",
      "  model_6/conv2d_84/BiasAdd (802.82k/802.82k flops)\n",
      "  model_6/max_pooling2d_32/MaxPool (802.82k/802.82k flops)\n",
      "  model_6/conv2d_82/BiasAdd (802.82k/802.82k flops)\n",
      "  model_6/conv2d_83/BiasAdd (802.82k/802.82k flops)\n",
      "  model_6/max_pooling2d_33/MaxPool (401.41k/401.41k flops)\n",
      "  model_6/conv2d_85/BiasAdd (401.41k/401.41k flops)\n",
      "  model_6/conv2d_87/BiasAdd (401.41k/401.41k flops)\n",
      "  model_6/conv2d_86/BiasAdd (401.41k/401.41k flops)\n",
      "  model_6/max_pooling2d_34/MaxPool (100.35k/100.35k flops)\n",
      "  model_6/conv2d_90/BiasAdd (100.35k/100.35k flops)\n",
      "  model_6/conv2d_89/BiasAdd (100.35k/100.35k flops)\n",
      "  model_6/conv2d_88/BiasAdd (100.35k/100.35k flops)\n",
      "  model_6/dense_20/Softmax (5.00k/5.00k flops)\n",
      "  model_6/dense_19/BiasAdd (4.10k/4.10k flops)\n",
      "  model_6/dense_18/BiasAdd (4.10k/4.10k flops)\n",
      "  model_6/dense_20/BiasAdd (1.00k/1.00k flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "FLOPS: 31.0 G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30960211824"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "from keras_flops import get_flops\n",
    "\n",
    "# build model\n",
    "# inp = Input((32, 32, 3))\n",
    "# x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inp)\n",
    "# x = Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# out = Dense(10, activation=\"softmax\")(x)\n",
    "# model = Model(inp, out)\n",
    "\n",
    "# Calculae FLOPS\n",
    "flops = get_flops(my_VGG16_model, batch_size=1)\n",
    "print(f\"FLOPS: {flops / 10 ** 9:.03} G\")\n",
    "# >>> FLOPS: 0.0338 G\n",
    "flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24503457136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:03:49.868068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 249s 132ms/step - loss: 0.1658 - accuracy: 0.9501\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the training results by plotting the collected data in the\n",
    "`history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].set_title('Training loss')\n",
    "axs[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].set_title('Training accuracy')\n",
    "axs[1].set(xlabel='Epochs', ylabel='Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer mnist_classifier: expected shape=(None, 100, 100, 1), found shape=(None, 28, 28, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_929/1829572746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer mnist_classifier: expected shape=(None, 100, 100, 1), found shape=(None, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test,y_test)\n",
    "print(\"Test loss: {}\".format(loss))\n",
    "print(\"Test accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the entire model graph is saved as a set of weights in floating point precision. We can make sure of that by printing out the dtype of one of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization\n",
    "In order to compile the trained model for deployment on a DPU platform, we must first quantize it. Here we will use the `vitis_quantize` module to convert the floating point model into an INT8 quantized representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:09:46.207191: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-03-03 14:09:46.207236: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_model_optimization.quantization.keras import vitis_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize model**\n",
    "\n",
    "By default the `quantize_model` function converts the weights, activations and inputs into 8-bit wide numbers. We can specify different values and configurations using `weight_bit`, `activation_bit` and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1006/4213455992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvitis_quantize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVitisQuantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "quantizer = vitis_quantize.VitisQuantizer(model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset = x_test[1:1024], weight_bit=8, activation_bit=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate quantized model**\n",
    "\n",
    "In order to evaluate the quantized model, it needs to be re-compiled with the desired loss and evaluation metrics, such as accuracy. Since we are using 8-bit quantization we do not lose much performance, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantized_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1006/4035252226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantized_model' is not defined"
     ]
    }
   ],
   "source": [
    "quantized_model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "score = quantized_model.evaluate(x_test, y_test,  verbose=0, batch_size=32)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save quantized model**\n",
    "\n",
    "Once we are happy with the performance of the quantized model, we can save it as a .h5 file, simply using the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.save('tf2_mnist_classifier_quantized.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compilation\n",
    "\n",
    "For this final step we use the Vitis AI compiler `vai_c_tensorflow2` and pass the quantized model as a parameter. In this example we are compiling the DPU model targeting the KV260 board, however to target a different board you will just have to point the compiler to the right `arch.json` file. \n",
    "\n",
    "For example, for the ZCU104 you would pass\n",
    "\n",
    "`--arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU104/arch.json`\n",
    "\n",
    "and for Ultra96, we can pass the custom arch.json in this repository\n",
    "\n",
    "`--arch ./arch.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_mnist_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/deploy_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/DPU-PYNQ/host/tf2_mnist_classifier_quantized.h5\n",
      "[INFO] keras version: 2.6.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 10/10 [00:00<00:00, 8776.53it/s]            \n",
      "[INFO] infer shape (NHWC)  :100%|█| 18/18 [00:00<00:00, 5513.98it/s]            \n",
      "[INFO] perform level-0 opt :100%|█| 2/2 [00:00<00:00, 196.62it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 726.03it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 18/18 [00:00<00:00, 370.02it/s]             \n",
      "[INFO] dump xmodel: /tmp/deploy_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B2304_MAX_BG2\n",
      "[UNILOG][INFO] Graph name: mnist_classifier, with op num: 26\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/DPU-PYNQ/host/./xmodel/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/DPU-PYNQ/host/./xmodel/deploy.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 792424b1c91dfd4ce57a997a2371a50a, and has been saved to \"/workspace/DPU-PYNQ/host/./xmodel/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_mnist_classifier_quantized.h5 \\\n",
    "    --arch ./arch.json \\\n",
    "    --output_dir ./xmodel \\\n",
    "    --net_name deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=3\n",
    "os.rename(\"./xmodel/deploy.xmodel\",\"./xmodel/mnist_{}.xmodel\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Copyright (C) 2021 Xilinx, Inc\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0 License\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
