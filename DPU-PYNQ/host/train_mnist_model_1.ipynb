{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a CNN for DPU compilation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim/s\n",
    "\n",
    "In this notebook we show how to train a simple Convolutional Neural Network (CNN)\n",
    "on the MNIST handwritten digit dataset for deployment on the DPU. We will cover:\n",
    "\n",
    "* Loading and pre-processing the imagenet dataset\n",
    "* Training a CNN with Tensorflow2\n",
    "* Quantizing and evaluating the quantized model\n",
    "* Compiling for DPU using the Vitis AI compiler\n",
    "\n",
    "## References\n",
    "\n",
    "* [Vitis AI model zoo](https://github.com/Xilinx/Vitis-AI/tree/master/models/AI-Model-Zoo)\n",
    "\n",
    "## Last revised\n",
    "* Dec 13, 2021\n",
    "    * Initial revision\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 16:17:19.166509: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-03-28 16:17:19.166541: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset\n",
    "\n",
    "The MNIST dataset comes with 60k training and 10k test examples that are\n",
    "28x28 grayscale images, along with their labels which are just the \n",
    "corresponding digits saved as integers. We can use the `keras.datasets`\n",
    "utility to load the MNIST dataset straight into our Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (60000, 28, 28), (60000,)\n",
      "Test data: (10000, 28, 28), (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Training data: {}, {}'.format(x_train.shape, y_train.shape))\n",
    "print('Test data: {}, {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "uint8\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n",
    "x_train = x_train[0:100]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAABRCAYAAAAgnzvqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuBElEQVR4nO2deXyV5Z3ov89ZcpKTc7IRICSQkIQsQEJYDZAIqCDLCGgRBWmdar06bWdsq1ar9k69c+tMP7dOe0dndPSKtLV1XIpSXBCQgCIUCJCwBRKWBMi+7yHJOXnuHyc5E0JehCRnSc7z/XzeDznb+/zeL+/ye99nE1JKFAqFQqFQKBQKxX+j83QACoVCoVAoFAqFt6GSZIVCoVAoFAqFog8qSVYoFAqFQqFQKPqgkmSFQqFQKBQKhaIPKklWKBQKhUKhUCj6oJJkhUKhUCgUCoWiD16VJAsh9gghHnH3b4cDyo02yo02yo02yo02yk3/KC/aKDfaKDfaeLsblyTJQogiIcRiV6x7KBBCfFcIYRdCNPdaFrmpbK92AyCE+IkQolwI0SCEeEsIYXJTuV7vpgchRJYQQgohDG4qz6vdCCFShBDbhRDVQgi3Dr4+DNyYhBC/FUKUCiHqhBCvCiGMbirb2938rRDiiBCiUQhRLIT4P+44poaBF3U8aSCEWCeEyO++PlUKIX4vhAhyU9ne7kbtNzfAzVy/vepJspv5q5TS0mvZ4+mAvAEhxFLgZ8AdwEQgDvhfnozJ2xBCbADckhwPIzqB94HveToQL+RnwGwgBUgEZgI/92hE3oMZ+DEQDqTjOO885cmAvAR1PGmzD8iQUgbjuD4ZgF96NiSvQe0338DNXr/dmiQLIUKFEJ8IIaq6n6h8IoQY3+dr8UKIQ913iX8RQoT1+v1cIcR+IUS9EOKYu57+ugMvcvO3wEYp5SkpZR3wv4HvDnBdQ4IXuUEIEQz8Anh6oOsYSrzFjZQyX0q5ETg18K0ZWrzFDbASeFlKWSulrAJeBh4e4LqGBG9xI6V8TUq5V0rZIaUsAf4EZAx4wwaJF3lRx5MGUsrLUsrqXm/ZgUkDWddQ4UVu1H5z/Vhu+vrt7ifJOmATEANEA23Av/f5zoM4LiCRgA3HBQUhRBTwKY47xjAcTxs2CyFG9y1ECBHdLTT6OrHMEI4qiQIhxP8Ubqo2vw7e4mYqcKzX62PAWCHEqAFu11DgLW4A/hl4DSgfzAYNId7kxtvwFjeie+n9enz3CdtTeIubvizAsxd4b/XiDXiNGyFEphCiAWgC1gD/d1BbNni8xo0X4k1ubv76LaUc8gUoAhbfwPemA3W9Xu8BftXr9RSgA9ADzwBv9/n9duBve/32kRuMLw6IxfGflwrkAc+6wsUwdHMeWNbrtRGQwETlhtlALo6qmondXgxqv7nq95McpxXXOxkubnCc4PcBo4EI4GD3vjPO1930WcdDQDEQrrw4f6+Op+vHEAW8ACQqN2q/+YZyB3T9dndzC7MQ4nUhxEUhRCPwFRAihND3+trlXn9fxJGkheO4C1nbfadQL4SoBzKBcTcbh5TygpSyUErZJaU8AfwTcO8AN2tI8BY3QDPQuxNEz99NA1jXkOANboQQOuBV4EdSStsgNmdI8QY33ooXuXkRyMFxgt4PbMHRdrByAOsaErzITU88dwO/ApbLq6vS3Yq3efEmvNGNdDTR+Rx4dzDrGSze6MZb8AY3g7l+u7uJwZNAEpAupSwXQkzHcfHoXRU5odff0TguJtU4JL4tpfwfLohL9onBE3iLm1NAGo7G/3T/XSGlrBmCdQ8Ub3AThONO9D0hBDjucgGKhRBrpZR7B7n+geINbrwVr3AjpWwD/r57QQjxKHBESmkf7LoHgVe4ARBCLAP+H/A33Q8tPInXePFCvNWNAYh3wXpvBm914w14g5sBX79d+STZKITw77UYACuO9ij1wtEw+xf9/O7bQogpQggzjie8f+6+mPwRWCmEWCqE0Hevc5G4tgH4NyKEWC6EGNv9dzLwP4G/DHA7B4LXugH+AHyvu5xQHL3wfzeQjRwg3uqmAUd7qendy4ru92fhqD53B97qBuHAH/Drfu0v3DR0YDfe7CZKCBHZ7WgujvNNf7G4Cm92czuOznprpJSHBryFA8ObvajjSQMhxAbhaH8qhBAxOGpqdg14S28eb3aj9pv+Gfj124VtU2Sf5ZfdQe7BUaVfADxGr3Yh3Z/9C3AIaAQ+plf7NBxDBH0J1AJVOBp0R/dtm4LjTqS557N+4nsJqABagAs4/lOMrnAx3Nx0f+eJbj+NOBrcm5Sba2KdiPvbJHutm14+ei9Fyo0ER2e0IqAVyAc2uMPLMHGzG0dHneZeyzblRR1P13HzIo626y3d/74BjFJu1H5zPTcanr7x+i26f6BQKBQKhUKhUCi68eXJRBQKhUKhUCgUin5RSbJCoVAoFAqFQtGHQSXJQohlwjGH+jkhxM+GKqiRgHKjjXKjjXKjjXKjjXKjjXKjjXKjjXLTP77mZcBtkoVjjLsCYAmOxvPZwHopZd7QhTc8UW60UW60UW60UW60UW60UW60UW60UW76xxe9DOZJ8i3AOemYmKMDx2Deq4cmrGGPcqONcqONcqONcqONcqONcqONcqONctM/PudlMJOJRHH1LCnFOIbquArhGDj/0e6XswZR3lBSLaW8Zu7vIUS50Ua50Ua50Ua50WbYupFSunoSp290441ecP0+A8rN9VBu+mfYnmsYoJvBJMn9ndyuabshpXwDxziGCCG8Zby5iy5ev3KjjXKjjXKjjXKjzXB242q+0Y2XenH1PgPKzfVQbvpnOJ9rBuRmMM0tirl6KsHxQOkg1jeSUG60UW60UW60UW60UW60UW60UW60UW76x+e8DCZJzgYShBCxQgg/YB2wdWjCGvYoN9ooN9ooN9ooN9ooN9ooN9ooN9ooN/3jc14G3NxCSmkTQvw9sB3QA29JKU8NWWTDGE+6MRgM+Pv74+/vj9FopK2tjZaWFmw2W890jB5F7TfaKDfaKDfaKDfauMuNn58fgYGBmM1mOjs7qa+vp7Oz0yvOuVqo/UYb5aZ/vNmLwWAgJCQEgJqamqE79twxp3ev+bL7zuvtqeWwO7fbnW6mTp0qn3rqKZmVlSWbmprkpk2b5Ny5c2VgYKDPu1H7jXKj3HiXG097GCovmZmZ8u2335ZSSnnixAmZmZkpLRaL2mfU8aTcuGnbp02bJnft2iWzsrJkWFjYkLkZFjPuCSEwGo2EhoY6l1WrVvH888/z4osv8s477xATE8MvfvELDh48SENDA5cvX+bZZ58lMDDQ0+G7jZSUFDZu3MjPf/5zMjIyCAgIYP369WzevJnJkyd7OjyvZfr06fz2t79ly5YtxMfHezocj2OxWHj66aeprKyksbGRV199lZSUFE+HpfBCzGYzo0aNIi0tjYceeoinnnoKi8WCEK4etMJ7CA8PZ86cOSxfvhwpJUlJSSQkJBAcHOzp0DxOREQEiYmJzJw5k0cffZT6+noaGhr6XbZt28a9996LwTCY8QSGF0IIgoKCWLBgAdu3b2fChAnf/CPFNSxevJinnnqKmTNncv78eex2+5Ct2+v2RqPRSFhYGAaDgcmTJzNp0iTCw8MZNWoUc+fOdX4vODjYmQC3tLTwox/9iCVLljB+/Hg6OzspKSkhLy8Pm83mqU1xGyaTiYSEBH7961+TnJxMYGAgXV1dtLa2YrPZGDVqFElJSRQXF1NVVTWkO5AnMBqNTJkyBavVyvHjx2lsbBzU+mJjYzGZTFRVVfnUxb0/IiIiuO222/je975HSEgIer0ek8mEXq/3dGgKL8FgMDB27Fjuu+8+pkyZQnR0NOHh4VitVgwGA3q9nrfffpvS0hHdn8dJa2srZWVlnDt3jjlz5qDX69Hr9T57LhFCEBMTwx133EFGRgYRERGYTCaio6OxWq2av5s7dy6dnZ0EBgbywQcf0Nra6saoPYPRaCQuLo5///d/x2KxMGbMGCorK2lvb/d0aMOGu+++mzVr1rBo0SLsdjvZ2dl0dnYO2fq9Kkm2WCwkJSWxdu1azGYzkZGRjB07FqvVSmBgIHFxcdf8prW1lV27dtHY2MihQ4c4dOgQdXV1XL58mdzc3BGdJOv1eiwWC7GxsTz44IOkp6cTGBiITqejra2N4uJiDhw4wAMPPMC6desYNWoUX331Fbm5uZ4OfVAYjUbS0tKIioqiqKho0ElyREQEISEhtLW1DVGEwxeLxUJCQgIxMTHOi3x3lZnPEBgYSEREBMnJyc4EUK/XU1FRQU5ODs3NzUyYMIHz589TVlZGU1OTp0N2C1arlejoaFJSUrjlllvIzMwkMjISf39/hBB0dXURFhbGihUr+Prrr6mvr/eJRKe9vZ2amhrKyso8HYpXYDQaWb16NStXriQhIYGgoCD0ev1Vtbr9nVOsViuzZs2iqamJvXv3cuHCBXeG7RF0Oh2BgYGkpKTQ0tJCWFgYJpNJJck3wbRp00hNTSUoKIhz587x9ddf09HRMWTr96okOSAggISEBL73ve8RFhbW73eampqw2+0EBARgNBppamri3Xff5cqVK4AjaW5qaqK5uZmSkhJ3hu92JkyYQHJyMnPnzuWBBx4gKCjImdh0dXXR1NTE9u3bWbZsGUuXLsVqtdLW1jYikuT09HSsVitms3lQ6zKZTMTGxjJmzBguXbrk3I98kYCAACIiIkhISECv19PW1kZhYSGFhYW0tLR4Ojy3EBQURGJiIhkZGSxcuJBp06YRHByMn58fly5dIisri9raWlJSUti1axd79uzhzJkzng7bpej1eue5Zs6cOSxcuJCMjAyMRiMdHR3k5eVx+fJl9Ho9K1asIDU1lejoaM6ePesTSbKUEpvNNuxr6IYKvV7PzJkzSU9PJyAggK6uLtrb27l06RJCCGeCbDAYsFgsVz1dHjt2LMnJyYwePdonkuQehBBYLBYsFgtGo9HT4XgFfn5+BAUFERERQX5+fr9Ph8PDw4mPjyckJISamhr27t1LXt7QzpDtVUlyU1MT5eXlVFVVYbVa6erqwmg0otM5mk7b7XaOHDlCQ0MDkyZNIjIykpqaGnbs2OHhyN2PTqfj3nvv5f7772fmzJlIKbHb7djtdufJJzY2Fj8/Pw4fPkxGRgZhYWGMGzfO06EPGn9/f5YuXcqhQ4cGva6oqChuv/129Ho9u3btori4eAgiHH7odDoSExNZsmQJd911FwDnz5/nmWee4fDhwz7xtFSn0zFr1izWrVvHmjVrsFqtdHR0cOjQIXQ6HREREfzd3/2dsyo9ODiYioqKEZ0kCyEICQnhxz/+MXfeeScxMTGYTCbAkRwWFhbyn//5n+zcuZPx48ezYsUKgoKCCAkJcX5vpNNzcxkTE+N8T6fTodPprkoKfQWbzcb+/fu57bbbsNvtzlrNrVu3XuUjNDSU1NRUFi1adNXDHXXDoQDHDVNmZiYPPfQQ3/nOd6iurr5mv1i6dCkpKSlIKcnOzuaNN94Y8ji8Kkm+cuUKJ06c4B//8R9ZtmwZJ0+e5Lvf/S6pqalcuXKFU6dOcc8999Dc3ExERARz5swhIyPD02F7hJSUFBYuXMiUKVOc7/31r38lKyuLpUuXMn36dMrKyjh+/Dj19fVMmjQJIcSIaSfXc+M0WF566SXi4uLIycnxmTaU/TF9+nQef/xx7rrrLiwWCwCvvfYaubm51NbWejg612MwGEhLS+OZZ55hwYIF2Gw29u7dy7/8y78427jNnz+f559/ngULFgCQn58/4qvYDQYDK1eu5Dvf+c41HdEKCwt59NFHyc3NRQjB+PHjPRSlZ/Hz8yMsLIyIiAjne1OmTOHChQu0trZSXV3twejcT2dnJxs3buSTTz5Bp9M5nyT39WAwGJg4cSJZWVmMGzcOIQT5+fns2LGD48ePeyh6z9Bz42AymXyq4+L1yMzM5MUXX2T8+PEkJydz6NCha5pEPv744yQnJ7N161b+4z/+Y8ifIoOXJckAtbW1fPrppxw4cICGhgYMBgNr164lMjKS119/nebmZrq6uigvL+fzzz8nKyvL0yG7FZ1Ox+TJk9m4cSOJiYmYTCaam5vZv38/3//+90lKSqKxsZG3336bL774gqKiIs6cOcPTTz/NzJkznYm1K3Ymd2C1Wp3VcUOR8FutVvR6PUVFRcO+GcpgWLp0KTNmzCAkJITOzk4OHz7Mn//8Z+rr6z0dmssxGAysWbOGX/7yl0RFRVFdXc2XX37J888/T3l5OZ2dncyYMYM777zT2Xm4vLyc7du3c/r0aQ9H71oMBgN33XUX/v7+ADQ3N1NWVkZeXh4vvvgiZ86coa2tjTFjxhAbG+vhaD1DfX09R44c4fPPP+fhhx8G4OGHH8ZisSClZNeuXR6O0P3YbLarbiD7e5o+evRoMjIyCAkJcZ7Lm5ubqa6uHtI2pcOJpKQkcnNzqaio8HQoHsdut9Pe3o6UEpPJdNX13mAwkJSUxIQJEzAajc4O5q7A65JkKSVtbW2Ul5djs9moq6ujubkZs9nMvHnz+P3vf09XV5fz7tSXGrj7+fkxbtw41q5dS1xcHEIIqqurKS4uZvv27ZSUlNDc3Oxsv1NZWYnNZnNOJNLTw3j16tXDNkkOCwtj5syZzs5CA6WnujwiIgI/Pz+ampqoqakZwkiHD1arlcWLFxMTE4Pdbqe6upo//elPNDU10dXV5enwXMqYMWP4m7/5G374wx8yYcIEjh8/zs6dO/n4448pLS11dvxNT09n0aJF+Pn5AfDWW2+Rl5dHc3OzJ8N3OT1PBRsbGykvL+fSpUuUlZVRXFzsTJB7zi1a/UhGOlJKzpw5w5YtW5xJstlsxmw2O/cXX+R6545JkyaRnp7O0qVLnTdgAMeOHWP79u3uCM8r6GnP3tLSQmBgIOPGjXPW5PkymZmZ3HbbbYSHh1NcXMzFixedbZJ1Oh0hISHcf//9WK1WSkpKyMnJ4eTJky6JxeuS5B56Lk5nzpzhxIkTTJo0iXnz5jF+/HiKi4tH9KgV/SGEYNSoUSxdupTly5fj5+fHkSNHOH36NIWFhWRnZ2Oz2aiurtas3usZkzEtLc3N0Q8dVquVSZMmodfrqaurG/ATB5PJxG233caYMWO4cuUKjY2NPtM5rQedTofFYuGee+4hMTERi8VCVVUV2dnZZGVljfhjLDIykvnz57N+/XrS0tIoKSnh888/59NPPyUnJwebzYZeryciIoKUlBTi4+Pp7Ozk+PHjfPzxx5SXl4/4tpM97UttNhuVlZVUVlbS0NBwTYc8s9lMVFSUh6L0PPX19RQVFTnb3I6UZm1DzaRJk0hJSSEtLY20tDRSU1OdTecaGhq4ePEiFy9e9HCU7qMnQS4uLiYpKYmAgACfb25hNpu59dZbueWWW+jq6uKzzz6jrKzMeT0KDw9nwYIFLF++HLvdztatW/nyyy+pqqpySTxe/7+RnZ1NUFAQ8fHxLFmyhDvuuIO9e/dSV1dHW1vbiH+S04Ofnx+xsbF8+9vfZtasWZw9e5b33nuP3bt3U1FRgdFovKEOIkaj0Tl143DE39+f8PBwwNEm8mZqEoQQ+Pv7YzabGTduHA899BBhYWHk5eVRUlLiU7US4NgXYmNjefLJJwkPD6ejo4MLFy6wdetWCgoKPB2eS9HpdKSlpXH//fdz++23U1dXx86dO/nLX/7CsWPHsNvt+Pn5ERcXx7x585gyZQomk4nKykr++Mc/kpOTM6RjcXozDQ0N7Ny50/lar9cTFhZ2Vb+AqKgoEhISAGhra6O1tdVn/PTGFzvq9YfRaCQoKAiz2ewcY12n03HPPfdw3333ERcXh9VqdbZZ7ukAW1RUNOJvzntjt9tpbGzk3LlzJCYmejocj2MymZg2bRqZmZmMHz+ewsJCfve73zmHeQ0MDGTatGk8+OCDzJgxg5MnT/LWW29x7Ngxl8Xk9UlyR0cH+/bto7Ozk4yMDH7xi19w6NAhTp8+zcGDB9m2bduIf5oDjp6eM2bMYP78+UgpefbZZ9m7d6/PNhEAOH369DVPtHqe4PT9t/d4lPPnz2fx4sUsWLAAvV7PRx99xL59+3xiuKreWCwWFi5cSFJSEnq9nlOnTrF9+3Y2b97s6dBcjtlsZs6cOdxxxx3YbDa++uornn76aerr69HpdAQEBBAbG8ubb75JSkoKAQEBlJSU8Nlnn7Fx40afTADBcRyNGjWKDRs2EBgY6EwKIyMjnbNV5uTkkJOT43PtKqWUziYGvpwo63Q6YmJiWLNmDenp6YwePdr52bx58666kZBS0tTUxK9+9Su+/PLLq6rVFb5FzwRyr7zyCklJSVy4cIEtW7aQk5Pj/E5GRgYbNmxg2bJldHV18eabb1JeXu7auFy69iGiZ6KQ73//+7z66qusWrWK1atXc/LkSWJiYti8eTMVFRUj+sS0ZMkSHnvsMbq6uti3b99NJ8g9I1uMpBEuQkJCrqqa0ul0pKSkoNPpSE1NZeLEiYSGhmIymbjvvvswGAzOJimXL1+mo6MDg8FAfn6+yw80b2PixIksX76cF154AZ1OR05ODi+//DKffPKJTwz31jNMYlBQEM3NzTz55JMYjUbuu+8+FixYwOzZs50TIfRw6dIlPvroI5+7mQJHDU50dDS33Xab0xE4jrme825XVxdlZWX8wz/8AwUFBSP6fKzoH7PZTEpKCn/6058YP378NU0H+l57eh6Cbdq0ySePq76EhIQQEBDg6TDcihCCiIgIVq1axcsvv+wcJ3rs2LHcfvvtdHV18fLLL2O1WsnIyCA9PZ22tjY+/PBDNm7c6PJJwIZFkgyOXq/btm3jwQcf5LHHHmPOnDkkJSXxxBNPMHbsWN544w0qKipG5F3o9OnTmTNnDhMnTqSzs5OsrKwBNQ2QUtLS0jKsq9JtNptz25977jkeeOAB50Gi1+uZOnUqOp3OeaBduXKFy5cv88knn1BQUEBBQQFFRUXo9Xo+/fRT/P39KS0tpa6uzmPb5G7CwsJYuXIlzz77rLPKs7y8nLKyMhoaGjwdnluw2+20tLTQ1NSExWJhx44ddHV1OTtcGY1Gmpub6ejoIDQ0lPb2dgoLC9m3b5+nQ3cbQgiMRiOJiYn88z//M7GxsQQHB2MymaiurubYsWPMmjWLoKAgZzJkMBhYuHAhNTU1lJaWjviOn4prEUI4pyfvO1Rn75sqcFSvL1y4kFtvvZW//vWvg549dbiTnp5OZGSkp8NwK4mJiaxYsYKnnnrKeR6pqqpCCMHcuXNJSUlh9uzZjB49mtjYWEJDQ7l06RLPPfecWyb/GjZJcldXF42Njezfv5+uri6OHj3KrbfeSmZmJvfccw92u52NGzeOyMkggoKCCA4Oxmg0Ultby+7du284STYYDCxfvpywsDCqq6udQ3sNVy5fvsw777xDYGAgUVFRBAQEOHuQCyFobm4mLy+PioqKqxK/qqoqamtrqampwWQyMWfOHAIDA51t232hyU4Ps2fPZu7cuYwZMwZw3DxlZWVRWFjoM0lNW1sbX3/9NVFRUaxfv574+Hhqamo4f/48+fn5nD17lvr6eh5++GGsVisXLlwgLy/PZzp3CiGwWq1MnjyZn/70p8yfP5/8/Hyys7MpKiri/PnzNDc387Of/YzJkydjsVicHUEfeOABKisr2bdvH6WlpT5zbAkhnG1shRCMHTv2qrGTfYGOjg6Kior413/9V2699VYqKipoaGhw7gM9TS3i4+OZNm0aKSkpBAYGYrFYnG2XfY329nYKCgpYvnw5gYGBPjXjXnJyMnfffTff+ta3CA8Pp6mpiXfeeYfS0lJiY2NZtGgRMTExLFiwgICAAOeoVqNHj+bxxx/nlVdecY7i5SqGTZLcQ11dHXv27KGwsBCbzeYc93fNmjV8+eWX1NbWjrhqG71e7zyBNDU1cerUqRvaKUwmE8nJyc4pqwsKCtizZw8HDhxwdcguo6amhj179uDv709CQkK/k4rk5uZSVlZGeXk5lZWV13yenJzMvHnz8Pf358iRIyNuf7keQUFB3HLLLUyePNn5RKesrIyDBw+O+IkxemOz2cjJyXHO6mmxWKisrCQ/P58TJ05w7tw5AgMDeeyxxxBCUFJS4jO97nuqP1NSUli2bBl33303VVVV7Nmzhz179nD+/HlKS0tZsmSJs/q8Z4g4q9XKrFmzWL16NUFBQeTm5lJYWOis4WtpaRmRtX39ERMTQ3JyMhaLxWc6mPeMgvLuu+9y8eJFSkpKqK2tveZGaerUqTQ1NTF16lQPReo9dHR0cO7cOedrk8mEn5+fT4wVPXv2bBYsWMCECRM4dOgQ+fn5/P73v6empobJkycDsGrVKsLDw53nGiklwcHBLF26lNdff93lzUeHXZLcU41TUVHBpUuX6OrqQq/XEx8fT0JCAufOnRuxSY/NZqOpqekbmwbo9Xr8/f2dIzgsW7aM4uJi9uzZc1NPob2VK1eusGXLlgH/Pjg4mOTkZAC+/vprn2liADBt2jRmzZpFdHQ0Ukra29vZuXOnc3YwX6KnVmb37t3XfGY2m0lMTGTKlCnodDqqq6v7veEaifS0/Vu/fj2rVq2ioaGBvXv3sm3bNo4ePYoQgvj4eF544QUSExPp6Ojg+PHjbN++nUmTJrF8+XJWrFjBjBkzOHLkCFu2bHE+TTx+/LjLhmryNHa7nebmZmeb0okTJzJz5kzi4+Nd2vve25BSUl1dzccff6z5nfr6esaNG8ejjz7qxsi8E5vNRk1NDVJKDAYDwcHBBAcHj9jjpDdpaWmYzWb279/PO++8c1Wn8draWtrb20lNTSU0NJSGhgY6Ojro6upy1gSWlZW5/KZ7WCXJQgiSk5NZvnw5qamppKSkOJ+wnj9/nuzs7BHZ3KKHurq6G5phMDIyknnz5rF+/XpWrlzJp59+yjPPPMOZM2fcEOXw4osvvvCJWeV6eO2114iPj8doNNLW1sbRo0d54oknfL4tYF+EEPj5+TmrznNzc8nOzvZ0WC5Hr9ezYcMGHnjgAdLT06mtreWFF15gy5YtVFdXExoayty5c3n++eeZNm0aly9fZtOmTXz44Yfk5eVhtVp57733WLZsGbfeeiurVq3i/vvvB6C0tJRHHnmEHTt2eHgrXUNDQwN//OMfeeSRR5xPt6Kjo/nWt77lU0nyjZCYmKieIveiJ9ETQqDX631mrORDhw6xY8cOcnNzr7kpMJlMjBkzxjk03j/90z+RnZ1NXV0ddrud0tJStzzwGxb/E8HBwSQlJbFu3TpWrlzpnCWtp6rdbrdTUVExoqsnhBCEh4ezbNkynnnmGc3vbdiwgXXr1jF37lyMRiPvv/8+P/jBD1QSpAAcvaf9/PyQUlJXV8ebb75JU1OTGomgDy0tLezevdvnvHz729/mBz/4AXFxcZw6dYqXXnqJnTt3MnXqVBYvXkxGRgapqakEBgby0ksvsXnzZs6fP09DQwNSShobG8nKyuKrr74iNTWV22+/ndtvvx2AX/7ylxw/ftzDW+g6etqW+uJkIj21uWFhYeTm5l63Q9WqVat45JFHnCOk+DpNTU1s27aNgoIC4uPjWbx4MeXl5WzatMnTobmcjz766KqhE3sICAhg6tSp3HnnnVgsFi5evMhnn33G+fPnkVI6F3fg1UlySEgIM2bMIDMzkwULFjBlyhRGjRrlbNje1NTE2bNn+eCDD/j8888pLCz0cMSuQ0qJ0WgkKiqKH//4x3zxxRfU1tYSFBREQkICmZmZxMfHOzvRVFVVUVBQwPvvv09zc7PPdMi6GYQQREVF+cTUsX5+fjz55JMEBwc7e5i3t7c72+UqriYgIIAZM2b4XLLz0EMPER0djcFgYMyYMdx///2sXr2ahIQERo0aRXBwMO3t7WzatInXX3+dyspKrly5ctU+ZLfbsdvtzkl6PvnkEwAuXrzo8uGaPEljYyN//vOfefzxx4mMjMRgMDBhwgRWr17N66+/TllZ2Yi86UpJSeGuu+5i3rx5tLS08JOf/OSaJLmnU2d6ejo//OEPmT59OoGBgYDj5qK9vd2nz0OdnZ0cOHCAsLAwsrKyRmxtS1+0+lbNnTuXNWvWsHjxYmpqanj88ccpKSnxSCdgr0uSdTodZrOZ8PBwVq9ezezZs0lJSSE2Nhar1QrgnJ3nxIkTHDlyhAMHDnD27NkReQLqjRACi8XCunXriIuLcybJ8fHxJCUlMXbsWJqbmykqKiInJ4cdO3aQk5PjM73LB0JAQMCI71VtNBqJjIxk+fLlmEwm53i2hw8f9rnxoW+Untn2fC1J7hl7XKfTERQUxKxZs/Dz86O2tpbCwkJKS0spKChg165dzj4hWrS2ttLa2uozHUJtNhulpaVcunSJ8PBwDAYDJpOJsWPHjuixb9euXcudd95JQkICFRUVLFq06Jox/E0mE4mJiWRmZjJjxgxCQkJoa2ujuLiYw4cPU1BQ4JbhvLwVKSX19fV0dHQ4R2byVYKCgpg/fz633HILZrOZPXv2sG/fPo/dYHtNkqzX6wkPD2fcuHGMGzeOxMREHnnkESZOnIjZbHYOAVdcXExOTg67d+/m4MGDnD59esQnxy0tLTQ2NtLW1kZAQIBzooPW1lYCAgIICQmhs7OT6upqsrKyOHXqFNnZ2f12SFL8N1JKgoKCRnySbDKZiI6OJjk5Gb1eT3t7O3l5eXz88ceqGc516G/klJHOgQMHaG9vd9bY9QzpdfDgQfLz8zl9+jT5+flUVVX59JM/LaSUHD9+nKlTpxIQEOAT01QvWbKEqVOnYjabsdvtrF279pqEJiAggJSUFBISEujs7KSxsZHz58+ze/duZzW6epjz3zPDBgYG+uy5OT09nfnz5xMREUFhYaGz46+njqNvTJKFEBOAPwARQBfwhpTy34QQLwD/A+hpbf2clPKzmw1Ar9fj5+dHSEgIq1at4t5772XSpElER0c7v9Mzv3l2djavvvoq+/fvp76+3uMHlavd9HDhwgWOHTtGWloaU6dOdfoKCQnBbrfT2tpKVVUVmzdv5te//rVX9Ip1l5vBMnnyZMxms1vLdLcbvV5PYGAgoaGhCCFobGzkwIED/Nd//ddgVz3keMt+c+XKFXJzc50nZp1O5/Gnyu5w8+STT5KRkcGUKVMICwujqqqKd999l8bGRjo6Orwy4fOWfQYcSfLWrVtZsWIFISEhrizqhnCHm7a2Njo6OvD392fUqFHcfffd13yn54aqs7OT8vJycnJy+PTTT/nDH/7gsb5E3rTfgKPGz2w2k5SURGJiIocPH3Z1kZp40s3zzz/P9OnTuXz5Mlu3buV3v/vdUK7+prmRJ8k24Ekp5VEhhBU4IoTY2f3Zb6WULw20cKPRyMyZM/nRj35EWloaUVFRziYVPXR0dJCTk8MHH3zAG2+84W2D+bvMTW+qq6vZtGkT+/btY+3atfz0pz91fnby5Ek+/PBD3n333avGWvQC3OJmMHgw6fF6Nx7EK9z0PG0/d+4csbGxhIeHM3r0aE8PF+hyNy0tLezYsWO4tYn0in0GHElyfn4+BQUFBAcHExoa6q6itXC5m5/85CesXbuWFStWkJaWds3njY2NVFZW0trayr59+3jvvffIy8ujtrZ2sEUPFq/Zb3Q6HXfffTf+/v6UlJR4QxMlj7n58MMPGTVqFJcvX2bv3r2uKuaG+cYkWUpZBpR1/90khDgNRA1F4f7+/s4xNc1mMzqdjtbWVoqKitizZw92u51t27Zx6tQp6urqvG4cV1e66UtLSwvHjx/n7Nmz/Nu//ZvzfZvNxpUrV7yuQ4w73dwsFRUV7Nu3j7vuussj5bvbTVtbG2fPnuXw4cPMmDHDVcUMCd6033R1dfHKK6/w85//nGXLlmGz2di4cSNFRUWeCMer3HgT3ualpKSEDRs2YDQaEULQ1dXlHAfX3bjDTV5eHr/5zW/Yu3cvK1euZMOGDQQFBbF7926OHj3K0aNHyc3Npaamho6ODq5cueIVE8p4034jpeTgwYNMmjSJ2tpaj4/U5Uk3b775Ju+88w52u90rHoqKmzlwhRATga+AFOAJ4LtAI3AYx13HdWe5EEJcVZjBYCApKYm4uLirhnNraWmhoqKCrq4uysvLaWxsHOr2b0eklLOHcoVD7caDjHg3AQEBREZGkpqaSlFREWfPnr3Rg3FYuhFCEBAQwMyZMwkNDaWzs5OioqKhHjd7WLr5JpKSknjppZeYOnUqFRUVfPjhh7z88ss3Oz7niHQzFEgph7Q6Z6R4YZjtM0IIQkJCGDduHDExMfj5+VFRUUFdXR11dXXOTmlDxLByc4PlM2vWLAIDA7l48SKlpaUD9TXi3AwhA3PTe8y56y2ABTgCfKv79VhAD+iAF4G3NH73KA6ZhwHpJcvhG91u5Ua5UW58142/v7986KGH5Pvvvy9zcnLk9u3b5dixY2X3id+n3QzForyofUa5UW682c2NijIC24EnND6fCJy8gfV4WtKQ70jKjXKj3IxsNwEBAXLVqlXytddek7t375ZxcXEeS5K9zc1gF+VF7TPKjXLjzW5uZHQLAWwETkspf9Pr/XHS0W4F4B7g5Deta6Sh3Gij3Gij3GjjjW7a2trYunUrW7dudVeR/eKNbrwB5UUb5UYb5UYb5ea/+cY2yUKITGAvcALHUCAAzwHrgek4MvQi4LFe8rTWVQW0ANWDCXqAhPcqN0ZKOXqwKxxiN01A/mBjGiDKjTbKjTbKjTbKTf94uxd1jdJel3KjvS7lRntdw9rNTXXcGwqEEIflEDcs9+ZybxRPxqfceGfZN4Jyo41yo406D2uj3Gij3Gij3GgznN343pRSCoVCoVAoFArFN6CSZIVCoVAoFAqFog+eSJLf8ECZniz3RvFkfMqNd5Z9Iyg32ig32qjzsDbKjTbKjTbKjTbD1o3b2yQrFAqFQqFQKBTejmpuoVAoFAqFQqFQ9MFtSbIQYpkQIl8IcU4I8TMXljNBCLFbCHFaCHFKCPGj7vdfEEKUCCFyu5cVrorhZlFutFFutFFutFFutFFutFFu+sddXrrLUm60y1JutMtyjZuhmp3lG2Zc0QPngTjADzgGTHFRWeOAmd1/W4ECYArwAvCUO7ZXuVFulBvlRrlRbkaCG3d6UW6UG29z464nybcA56SUF6SUHcC7wGpXFCSlLJNSHu3+uwk4DUS5oqwhQrnRRrnRRrnRRrnRRrnRRrnpH7d5AeXmeig32rjKjbuS5Cjgcq/XxbjhP1YIMRGYARzsfuvvhRDHhRBvCSFCXV3+DaLcaKPcaKPcaKPcaKPcaKPc9I9HvIBycz2UG22G0o27kmTRz3suHVZDCGEBNgM/llI2Aq8B8TimVCwD/tWV5d8Eyo02yo02yo02yo02yo02yk3/uN0LKDfXLVS50S50iN24K0kuBib0ej0eKHVVYUIIIw5Jf5JSfgggpayQUtqllF3A/8NRFeANKDfaKDfaKDfaKDfaKDfaKDf941YvoNxcD+VGG1e4cVeSnA0kCCFihRB+wDpgqysKEkIIYCNwWkr5m17vj+v1tXuAk64ofwAoN9ooN9ooN9ooN9ooN9ooN/3jNi+g3FwP5UYbV7kxDE1410dKaRNC/D2wHUePx7eklKdcVFwG8B3ghBAit/u954D1QojpOB73FwGPuaj8m0K50Ua50Ua50Ua50Ua50Ua56R83ewHl5nooN9q4xI2acU+hUCgUCoVCoeiDmnFPoVAoFAqFQqHog0qSFQqFQqFQKBSKPqgkWaFQKBQKhUKh6INKkhUKhUKhUCgUij6oJFmhUCgUCoVCoeiDSpIVCoVCoVAoFIo+qCRZoVAoFAqFQqHog0qSFQqFQqFQKBSKPvx/xCliqq/V4O8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(10, 10))\n",
    "plt.tight_layout()\n",
    "\n",
    "for i in range(10):\n",
    "    axs[i].imshow(x_train[i], 'gray')\n",
    "    axs[i].set_title('Label: {}'.format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will normalize the training and test images. We also need to add\n",
    "color channel. All these pre-processing steps are required by Conv2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 17:07:12.338720: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 12042240000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 224, 224, 1)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "x_train = tf.image.resize(x_train, [224,224])\n",
    "x_test = tf.image.resize(x_test, [224,224])\n",
    "print(x_train.shape)\n",
    "print(x_train.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the model\n",
    "\n",
    "Create a `tensorflow.keras.model` object by passing a list of layers. Because the MNIST dataset is not difficult, we can use a very simple network with a single convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 60, 60, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 58, 58, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 703,210\n",
      "Trainable params: 703,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(100,60,1))\n",
    "x = tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_classifier')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 32, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 2, 2, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 15,916,490\n",
      "Trainable params: 15,916,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "inputs = Input(shape=(32, 32, 1))\n",
    "\n",
    "# 卷积层和最大池化层\n",
    "conv1 = Conv2D(64, (3,3), padding='same', activation='relu')(inputs)\n",
    "conv2 = Conv2D(64, (3,3), padding='same', activation='relu')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=2)(conv2)\n",
    "\n",
    "conv3 = Conv2D(128, (3,3), padding='same', activation='relu')(pool1)\n",
    "conv4 = Conv2D(128, (3,3), padding='same', activation='relu')(conv3)\n",
    "pool2 = MaxPooling2D(pool_size=2)(conv4)\n",
    "\n",
    "conv5 = Conv2D(256, (3,3), padding='same', activation='relu')(pool2)\n",
    "conv6 = Conv2D(256, (3,3), padding='same', activation='relu')(conv5)\n",
    "conv7 = Conv2D(256, (3,3), padding='same', activation='relu')(conv6)\n",
    "pool3 = MaxPooling2D(pool_size=2)(conv7)\n",
    "\n",
    "conv8 = Conv2D(512, (3,3), padding='same', activation='relu')(pool3)\n",
    "conv9 = Conv2D(512, (3,3), padding='same', activation='relu')(conv8)\n",
    "conv10 = Conv2D(512, (3,3), padding='same', activation='relu')(conv9)\n",
    "pool4 = MaxPooling2D(pool_size=2)(conv10)\n",
    "\n",
    "conv11 = Conv2D(512, (3,3), padding='same', activation='relu')(pool4)\n",
    "conv12 = Conv2D(512, (3,3), padding='same', activation='relu')(conv11)\n",
    "conv13 = Conv2D(256, (3,3), padding='same', activation='relu')(conv12)\n",
    "pool5 = MaxPooling2D(pool_size=2)(conv13)\n",
    "\n",
    "# 扁平层\n",
    "flat = Flatten()(pool5)\n",
    "\n",
    "# 全联接层\n",
    "fc1 = Dense(1024, activation='relu')(flat)\n",
    "fc2 = Dense(2048, activation='relu')(fc1)\n",
    "\n",
    "# 输出层\n",
    "outputs = Dense(10, activation='softmax')(fc2)\n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 40)        3280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 6, 6, 40)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               184448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 189,018\n",
      "Trainable params: 189,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 04:56:44.522857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-02-22 04:56:44.522896: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-22 04:56:44.522927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-02-22 04:56:44.523136: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28,1))\n",
    "x = tf.keras.layers.Conv2D(40, (9,9), activation='relu', input_shape=(28,28,1))(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((3,3))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_classifier')\n",
    "model.summary( print_fn=None)\n",
    "model.count_params()\n",
    "model.get_config()\n",
    "model.output\n",
    "#model.get_layer(Conv2D)\n",
    "#model.compute_output_shape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 13, 13, 32) dtype=float32 (created by layer 'max_pooling2d')>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the model for training: choose desired optimizer, loss function and metrics to observe over the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 15:28:12.425072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/xilinx/xrt/lib:/usr/lib:/usr/lib/x86_64-linux-gnu:/usr/local/lib:/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib\n",
      "2022-03-03 15:28:12.425107: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_flops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/1362965469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_flops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_flops'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "from keras_flops import get_flops\n",
    "\n",
    "# build model\n",
    "# inp = Input((32, 32, 3))\n",
    "# x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inp)\n",
    "# x = Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "# x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(128, activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# out = Dense(10, activation=\"softmax\")(x)\n",
    "# model = Model(inp, out)\n",
    "\n",
    "# Calculae FLOPS\n",
    "flops = get_flops(model, batch_size=1)\n",
    "print(f\"FLOPS: {flops / 10 ** 9:.03} G\")\n",
    "# >>> FLOPS: 0.0338 G\n",
    "flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24503457136"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120/1875 [>.............................] - ETA: 24:56 - loss: 2.3031 - accuracy: 0.1013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112/3512106047.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/vitis_ai/conda/envs/vitis-ai-tensorflow2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the training results by plotting the collected data in the\n",
    "`history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].set_title('Training loss')\n",
    "axs[0].set(xlabel='Epochs', ylabel='Loss')\n",
    "\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].set_title('Training accuracy')\n",
    "axs[1].set(xlabel='Epochs', ylabel='Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0778 - accuracy: 0.9764\n",
      "Test loss: 0.07777635753154755\n",
      "Test accuracy: 0.9764000177383423\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test,y_test)\n",
    "print(\"Test loss: {}\".format(loss))\n",
    "print(\"Test accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the entire model graph is saved as a set of weights in floating point precision. We can make sure of that by printing out the dtype of one of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization\n",
    "In order to compile the trained model for deployment on a DPU platform, we must first quantize it. Here we will use the `vitis_quantize` module to convert the floating point model into an INT8 quantized representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.quantization.keras import vitis_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantize model**\n",
    "\n",
    "By default the `quantize_model` function converts the weights, activations and inputs into 8-bit wide numbers. We can specify different values and configurations using `weight_bit`, `activation_bit` and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI INFO] Update custom_layer_type: []\n",
      "[VAI INFO] Update activation_bit: 8\n",
      "[VAI INFO] Update weight_bit: 8\n",
      "[VAI INFO] Start CrossLayerEqualization...\n",
      "10/10 [==============================] - 5s 589ms/step\n",
      "[VAI INFO] CrossLayerEqualization Done.\n",
      "[VAI INFO] Start Quantize Calibration...\n",
      "32/32 [==============================] - 326s 10s/step\n",
      "[VAI INFO] Quantize Calibration Done.\n",
      "[VAI INFO] Start Post-Quantize Adjustment...\n",
      "[VAI INFO] Post-Quantize Adjustment Done.\n",
      "[VAI INFO] Quantization Finished.\n"
     ]
    }
   ],
   "source": [
    "quantizer = vitis_quantize.VitisQuantizer(model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset = x_test[1:1024], weight_bit=8, activation_bit=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI INFO] Update custom_layer_type: []\n",
      "[VAI INFO] Update activation_bit: 8\n",
      "[VAI INFO] Update weight_bit: 8\n",
      "[VAI INFO] Start CrossLayerEqualization...\n",
      "10/10 [==============================] - 3s 385ms/step\n",
      "[VAI INFO] CrossLayerEqualization Done.\n",
      "[VAI INFO] Start Quantize Calibration...\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "[VAI INFO] Quantize Calibration Done.\n",
      "[VAI INFO] Start Post-Quantize Adjustment...\n",
      "[VAI INFO] Post-Quantize Adjustment Done.\n",
      "[VAI INFO] Quantization Finished.\n"
     ]
    }
   ],
   "source": [
    "quantizer = vitis_quantize.VitisQuantizer(model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset = x_test[1:8], weight_bit=8, activation_bit=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate quantized model**\n",
    "\n",
    "In order to evaluate the quantized model, it needs to be re-compiled with the desired loss and evaluation metrics, such as accuracy. Since we are using 8-bit quantization we do not lose much performance, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3026034832000732, 0.07329999655485153]\n"
     ]
    }
   ],
   "source": [
    "quantized_model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "score = quantized_model.evaluate(x_test, y_test,  verbose=0, batch_size=32)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save quantized model**\n",
    "\n",
    "Once we are happy with the performance of the quantized model, we can save it as a .h5 file, simply using the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.save('tf2_mnist_classifier_quantized.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compilation\n",
    "\n",
    "For this final step we use the Vitis AI compiler `vai_c_tensorflow2` and pass the quantized model as a parameter. In this example we are compiling the DPU model targeting the KV260 board, however to target a different board you will just have to point the compiler to the right `arch.json` file. \n",
    "\n",
    "For example, for the ZCU104 you would pass\n",
    "\n",
    "`--arch /opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU104/arch.json`\n",
    "\n",
    "and for Ultra96, we can pass the custom arch.json in this repository\n",
    "\n",
    "`--arch ./arch.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_mnist_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/deploy_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/DPU-PYNQ/host/tf2_mnist_classifier_quantized.h5\n",
      "[INFO] keras version: 2.6.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 40/40 [00:00<00:00, 26124.60it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 67/67 [00:00<00:00, 830.14it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 2/2 [00:00<00:00, 254.97it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 989.69it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 67/67 [00:00<00:00, 67.60it/s]              \n",
      "[INFO] dump xmodel: /tmp/deploy_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B2304_MAX_BG2\n",
      "[UNILOG][INFO] Graph name: model_8, with op num: 125\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/DPU-PYNQ/host/./xmodel/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/DPU-PYNQ/host/./xmodel/deploy.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 5785d4e3369c5a25be7688abf2e4a143, and has been saved to \"/workspace/DPU-PYNQ/host/./xmodel/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_mnist_classifier_quantized.h5 \\\n",
    "    --arch ./arch.json \\\n",
    "    --output_dir ./xmodel \\\n",
    "    --net_name deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=3\n",
    "os.rename(\"./xmodel/deploy.xmodel\",\"./xmodel/mnist_{}.xmodel\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Copyright (C) 2021 Xilinx, Inc\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0 License\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
